---
title: "Fitness Device Weight Lifiting Measurements"
author: "Drew McWilliams"
date: "November 26, 2016"
output: html_document
---


## Overview

Fitness devices, such as Jawbone Up, Nike FuelBand, and Fitbit collect a large quantity of data on the movements of those who wear them.  For this study we have data for six individulas who were asked to perform barbell lifts in five different manners.  The desired outcome is to train a predictive model that will be able to use similar predictor variables in order to determine the manner in which the exercise was performed.


## Source Data
Two data sources have been provided for this project, one for model training, the other for testing.  The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har and the data sets are accessible via the following urls:

Training Data:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

Test Data:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The final selected model will be used to predict the manner in which the exercise was performed in the test data.  The test data set is comprised of 20 observations that do not include the classe variable, indicative of how the barbell lifts were carried out.

```{r warns, echo=T, message=F, warning=F}
library(caret)
library(randomForest)
library(Hmisc)
library(rpart)
library(ipred)


```

```{r DataGrab, echo=T}

set.seed(1123)
    trainurl="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    testurl="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#download and populate testing and training dataframes
    
    df_train<-read.csv(trainurl, header=T)
    df_test<-read.csv(testurl, header=T)
```

##Preparing the Data
```{r dim, echo=T}
#Size of the training set
dim(df_train)
```
The number of possible predictors is quite large, so I want to eliminate any extraneous column to reduce the chance of over training.  The first thing that I wanted to was to idenify and remove any columns that had a single unique value, and thus would be of little value.  With a very quick exploration of the data, I noticed many columns that were missing a majority of their values, so I eliminated these as well.  

With some initial model building I found a few others columns that led to overfitting.  These were any timestamp fields, as well as that fields, "X" and "num_window".  These were also removed from the training set.
```{r Remove, echo=T}
# clean up training - remove low varaiance and predominantly NA columns

    zVarcols<-colnames(df_train)[nearZeroVar(df_train)]
    naCols<-colnames(df_train)[colSums(is.na(df_train)) > nrow(df_train)*.5]
    time_cols<-colnames(df_train)[grepl("*time*", colnames(df_train))]
    rm_cols<-c(zVarcols,naCols,time_cols,"X","num_window")
    train_upd<-df_train[,!names(df_train) %in% rm_cols]
```

My resulting training data set was comprised of 54 variables, reducing the number of possible predictors by 106:

```{r traindata, echo=T}
dim(train_upd)
```

##Building the Models
With my remaining training set, I partitioned this into training and testing sets, with sixty percent allocated to training.  I also set a cross validation control at 3 that was used by all of the models that I built.


```{r datasplit, echo=T}
# create training and testing partitions of training data
    inTrain<- createDataPartition(y=train_upd$classe, p=.6, list =F)
    Part_train<-train_upd[inTrain,]
    Part_test<-train_upd[-inTrain,]
#Model Cross-Validation setting
    cv_set<-trainControl(method="cv", number=3, verboseIter=F)
```
I decided to try three different model methods to see which would have the best results.  For this exercise, I tried Random Forests, Gradient Boosting, and Bagged Classification and Regression method. The one with the lowest out of sample error will be used as to predict the outcome of the test set.

I built the three models, using the cross-valitation mentioned above, and stored the preditions to compare to the actual results.

```{r models, echo=T, message=F, warning=F}
#model builds
    mod_rf<-train(classe~., method="rf", data=Part_train, trControl=cv_set)
    mod_gbm<-train(classe~., method="gbm", data=Part_train, trControl=cv_set, verbose=FALSE)
    mod_trbag<-train(classe~., method="treebag", data=Part_train, trConrol=cv_set)
#generate predictions    
    pred_rf<-predict(mod_rf, Part_test)
    pred_gbm<-predict(mod_gbm, Part_test)
    pred_trbag<-predict(mod_trbag, Part_test)
```

## Model Review and Selection
**Random Forest**
The Random Forest method had very encouraging results.  The accuracy of this model was 99.3%, resuling in an out of sample error of .007.  The model took the longest processing time to build, but the resulting confusion matrix displays how few of the classe variable were actually missed.

```{r rf, echo=T}
confusionMatrix(pred_rf, Part_test$classe)
```
Training of the model resulting in the weighting of the variables in the importance illustrated below

```{r rfvar, echo=T, fig.width=7, fig.height=7.5}
plot(varImp(mod_rf))
```

**Gradiant Boosting**
The accuracy rate of the model built using Gradiant Boosting dropped a bit from that of the Random Forest.  The out of sample error was slightly higher, at .041.  With an overall accuracy 95.9%, this would still be a fairly reliable model.


```{r gbm, echo=T}
confusionMatrix(pred_gbm, Part_test$classe)
```

In the plot below, we can see that many of the variables have been dropped from consideration as compared to what was seen with the Random Forest.  We infer that this results in the lower accuracy.

```{r gbmvar, echo=T, fig.width=7, fig.height=7.5}
plot(varImp(mod_gbm))
```


**Bagged Classification and Regression**
With an accuracy of 98.34%, this model was a close second to the Random Forest model.  The out of sample error rate would be .0166.

```{r tbcf, echo=T}
confusionMatrix(pred_trbag, Part_test$classe)
```
A review of the variables that it used, one can see that the weighting is quite similar to that of the Random Forest.  It also is using many more variables that the Gradiant Boosting Model neglected.
```{r tbvar, echo=T, fig.width=7, fig.height=7.5}
plot(varImp(mod_trbag))
```

#Conclusion
The Random Forest Model was selected to predict the outcome on our initial test set.  I retrained the model, to include the entire testing set and run the predictions.

```{r Final_models, echo=T, message=F, warning=F}
#model retain
    mod_rf_final<-train(classe~., method="rf", data=train_upd, trControl=cv_set)

```

The result:
```{r predict, echo=T}
predict(mod_rf_final,df_test)
```


